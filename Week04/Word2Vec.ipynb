{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hmq-7x_y81wX",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Word2Vec Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQctzlnJ81wa",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "First, download the file from [https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?resourcekey=0-wjGZdNAUop6WykTtMip30g](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?resourcekey=0-wjGZdNAUop6WykTtMip30g) and place it in the same directory on your machine as this notebook.\n",
    "\n",
    "(If you're on colab, you'll need to upload this file to your colab project.)\n",
    "\n",
    "Note that this will download a giant 1.5GB file onto your computer!\n",
    "\n",
    "**DO NOT unzip/decompress/double-click on this file once it's downloaded!** (on some computers, this means that the original compressed file is deleted, but we need it in that format for this notebook to run.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "L-7oJpIl81wb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Run this every time you run the notebook \n",
    "embeddings_file = \"GoogleNews-vectors-negative300.bin.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, you'll need to install gensim by opening a terminal/command window and typing:\n",
    "\n",
    "`conda install gensim`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Odcg6Zo81wc",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Colab and own-computer users, everyone now continue here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PeAmdvB381wc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "PYfYx5my81wc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#This line will load the 200,000 most common words into wv, a variable of word vectors\n",
    "#Note that this will load these into memory, so if you don't have a lot of memory on your computer you may run into problems/slowness\n",
    "wv = KeyedVectors.load_word2vec_format(embeddings_file, binary=True, limit=200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gEY1sn4s81wd",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of a word vector: \n",
      "(300,)\n",
      "Word vector for 'language':\n",
      "[ 2.30712891e-02  1.68457031e-02  1.54296875e-01  1.27929688e-01\n",
      " -2.67578125e-01  3.51562500e-02  1.19140625e-01  2.48046875e-01\n",
      "  1.93359375e-01 -7.95898438e-02  1.46484375e-01 -1.43554688e-01\n",
      " -3.04687500e-01  3.46679688e-02 -1.85546875e-02  1.06933594e-01\n",
      " -1.52343750e-01  2.89062500e-01  2.35595703e-02 -3.80859375e-01\n",
      "  1.09863281e-01  4.41406250e-01  3.75976562e-02 -1.22680664e-02\n",
      "  1.62353516e-02 -2.24609375e-01  7.61718750e-02 -3.12500000e-02\n",
      " -2.16064453e-02  1.49414062e-01 -4.02832031e-02 -4.46777344e-02\n",
      " -1.72851562e-01  3.32031250e-02  1.50390625e-01 -5.05371094e-02\n",
      "  2.72216797e-02  3.00781250e-01 -1.33789062e-01 -7.56835938e-02\n",
      "  1.93359375e-01 -1.98242188e-01 -1.27563477e-02  4.19921875e-01\n",
      " -2.19726562e-01  1.44531250e-01 -3.93066406e-02  1.94335938e-01\n",
      " -3.12500000e-01  1.84570312e-01  1.48773193e-04 -1.67968750e-01\n",
      " -7.37304688e-02 -3.12500000e-02  1.57226562e-01  3.30078125e-01\n",
      " -1.42578125e-01 -3.16406250e-01 -7.32421875e-02 -5.76171875e-02\n",
      "  1.02050781e-01 -1.08886719e-01  1.24023438e-01 -2.50244141e-02\n",
      " -2.49023438e-01  1.25976562e-01 -1.79687500e-01  3.32031250e-01\n",
      "  7.14111328e-03  2.51953125e-01  4.34570312e-02 -4.34570312e-02\n",
      " -3.90625000e-01  1.76757812e-01 -1.13525391e-02 -1.97753906e-02\n",
      "  2.79296875e-01  2.36328125e-01  1.19140625e-01  5.59082031e-02\n",
      "  1.73828125e-01 -1.10839844e-01 -4.95605469e-02  2.13867188e-01\n",
      "  6.17675781e-02  1.38671875e-01 -4.45556641e-03  2.55859375e-01\n",
      "  1.80664062e-01  5.88378906e-02 -6.59179688e-02 -2.08007812e-01\n",
      " -1.19140625e-01 -1.57226562e-01  5.02929688e-02 -6.29882812e-02\n",
      "  5.00488281e-02 -7.27539062e-02  1.74560547e-02 -3.56445312e-02\n",
      " -1.93359375e-01  3.93066406e-02 -3.36914062e-02 -1.07421875e-01\n",
      "  5.78613281e-02 -8.20312500e-02  1.74560547e-02 -1.65039062e-01\n",
      "  1.46484375e-01 -3.08837891e-02 -3.86718750e-01  2.49023438e-01\n",
      "  8.74023438e-02 -2.15820312e-01 -4.10156250e-02  1.60156250e-01\n",
      "  1.85546875e-01 -2.27050781e-02 -3.73535156e-02  7.86132812e-02\n",
      " -1.46484375e-01  6.78710938e-02  1.26953125e-01  3.30078125e-01\n",
      "  1.11328125e-01  9.27734375e-02 -3.45703125e-01 -1.41601562e-01\n",
      " -5.29785156e-02 -1.50390625e-01 -7.81250000e-02 -1.27929688e-01\n",
      " -4.02343750e-01 -1.41601562e-01  8.44726562e-02  1.08398438e-01\n",
      " -4.44335938e-02  3.73535156e-02  5.61523438e-02 -1.91406250e-01\n",
      "  1.54296875e-01 -5.12695312e-02 -6.49414062e-02 -8.30078125e-02\n",
      "  7.17773438e-02 -1.33789062e-01  1.05468750e-01  3.33984375e-01\n",
      " -1.08398438e-01  1.91650391e-02  2.14843750e-01  2.15820312e-01\n",
      " -1.05468750e-01 -1.44531250e-01  4.32128906e-02 -2.71484375e-01\n",
      " -3.78906250e-01  1.09863281e-01 -8.15429688e-02 -6.12792969e-02\n",
      " -1.33789062e-01  9.71679688e-02 -1.04370117e-02 -1.21093750e-01\n",
      " -2.44140625e-01  1.02050781e-01  1.10839844e-01 -1.00585938e-01\n",
      "  1.71875000e-01 -3.61328125e-02 -4.39453125e-02  2.83203125e-01\n",
      " -8.93554688e-02 -1.70898438e-01  2.46093750e-01  1.16699219e-01\n",
      "  8.39843750e-02 -1.32812500e-01 -1.61132812e-01 -1.39648438e-01\n",
      " -8.59375000e-02 -1.37695312e-01 -9.32617188e-02 -1.33789062e-01\n",
      "  1.65039062e-01  4.93164062e-02 -1.21093750e-01 -2.11914062e-01\n",
      "  1.61132812e-01 -1.07421875e-01 -3.97949219e-02 -3.51562500e-01\n",
      " -5.02929688e-02  1.46484375e-01 -4.68750000e-02  4.17480469e-02\n",
      " -1.27929688e-01 -9.76562500e-02 -2.46093750e-01  6.78710938e-02\n",
      " -2.30468750e-01  1.80664062e-02  3.54003906e-02  7.32421875e-02\n",
      " -2.23632812e-01 -1.25976562e-01  2.12890625e-01 -3.93066406e-02\n",
      " -2.41699219e-02 -9.61914062e-02  7.51953125e-02 -1.46484375e-01\n",
      " -1.49414062e-01 -8.83789062e-02 -4.88281250e-02  2.32421875e-01\n",
      "  3.30078125e-01  1.59179688e-01 -2.35351562e-01 -1.25976562e-01\n",
      "  2.68554688e-02 -5.29785156e-02 -6.59179688e-02 -2.17773438e-01\n",
      " -6.37817383e-03 -2.53906250e-01  2.28515625e-01  4.93164062e-02\n",
      "  3.54003906e-02  1.66992188e-01 -7.27539062e-02 -2.53906250e-01\n",
      " -1.34765625e-01  3.69140625e-01  1.83593750e-01 -1.64062500e-01\n",
      "  2.26562500e-01 -8.88671875e-02  3.69140625e-01  5.54199219e-02\n",
      " -3.63769531e-02 -1.48437500e-01  9.13085938e-02  2.47955322e-04\n",
      "  2.67578125e-01 -1.63085938e-01  1.19628906e-01  2.77343750e-01\n",
      " -1.49414062e-01  1.33789062e-01 -8.25195312e-02 -1.74804688e-01\n",
      " -1.77734375e-01  2.06054688e-01  5.07812500e-02 -2.08007812e-01\n",
      " -1.74804688e-01  9.66796875e-02  6.98242188e-02 -5.79833984e-04\n",
      "  9.22851562e-02  7.95898438e-02  1.41601562e-01  8.72802734e-03\n",
      " -8.05664062e-02  4.80957031e-02  2.49023438e-01 -1.64062500e-01\n",
      " -4.66308594e-02 -2.81250000e-01 -1.66015625e-01 -2.22656250e-01\n",
      " -2.32421875e-01  1.32812500e-01  4.15039062e-02  1.15234375e-01\n",
      " -7.66601562e-02 -1.10839844e-01 -1.97265625e-01  3.06396484e-02\n",
      " -1.03515625e-01  2.49023438e-02 -2.52685547e-02  3.39355469e-02\n",
      "  4.29687500e-02 -1.44531250e-01  2.12402344e-02  2.28271484e-02\n",
      " -1.88476562e-01  3.22265625e-01 -1.13281250e-01 -7.61718750e-02\n",
      "  2.94921875e-01 -1.33789062e-01 -1.80664062e-02 -6.25610352e-03\n",
      " -1.62353516e-02  5.98144531e-02  1.21582031e-01  4.17480469e-02]\n"
     ]
    }
   ],
   "source": [
    "#View the actual value of a vector for a word:\n",
    "# this is a 300-dimensional vector\n",
    "print(\"Shape of a word vector: \")\n",
    "print(np.shape(wv['language'])) #play around with other words\n",
    "print(\"Word vector for 'language':\")\n",
    "print(wv['language'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "PAPGYXkr81wd",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7609457\n",
      "0.65996563\n",
      "0.3794182\n",
      "0.033181373\n",
      "0.04996342\n",
      "-0.0038632825\n"
     ]
    }
   ],
   "source": [
    "#We can compare words by computing their cosine similarity\n",
    "# Try this out for different word pairs:\n",
    "print(wv.similarity('dog', 'cat'))\n",
    "print(wv.similarity('dog', 'terrier'))\n",
    "print(wv.similarity('dog', 'dolphin'))\n",
    "print(wv.similarity('dog', 'aubergine'))\n",
    "print(wv.similarity('dog', 'exuberant'))\n",
    "print(wv.similarity('dog', 'economics'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "OiY0EO0781wd",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[('dogs', 0.8680489659309387),\n ('puppy', 0.8106428384780884),\n ('pit_bull', 0.7803961038589478),\n ('pooch', 0.7627376914024353),\n ('cat', 0.7609457969665527),\n ('golden_retriever', 0.7500901818275452),\n ('German_shepherd', 0.7465174198150635),\n ('Rottweiler', 0.7437615394592285),\n ('beagle', 0.7418621778488159),\n ('pup', 0.7406911253929138)]"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look for most similar words to a given word\n",
    "#Try this out for different words\n",
    "wv.most_similar(positive=['dog'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "rqzhpqYD81we",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[('Brussels', 0.59405916929245),\n ('Berlin', 0.592768669128418),\n ('Amsterdam', 0.5912652015686035),\n ('Madrid', 0.5895830392837524),\n ('Frankfurt', 0.55706387758255),\n ('Parisian', 0.5517574548721313),\n ('Rome', 0.5313206911087036),\n ('Stockholm', 0.5250915884971619),\n ('Marrakesh', 0.5232300758361816),\n ('Strasbourg', 0.5227499604225159)]"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look for most similar words to a set of words\n",
    "#wv.most_similar(positive=['potato', 'yam', 'yucca'])\n",
    "#wv.most_similar(positive=['croissant', 'donut'])\n",
    "wv.most_similar(positive=['London', 'Paris']) #Or maybe Camberwell?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "dasAHs1e81we",
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[('Ken_Livingstone', 0.7916719317436218),\n ('George_Osborne', 0.7817354798316956),\n ('Tony_Blair', 0.7768580317497253),\n ('Nick_Clegg', 0.7726122736930847),\n ('Alistair_Darling', 0.7554317712783813),\n ('Mr_Clegg', 0.7268158197402954),\n ('Chancellor_Alistair_Darling', 0.7248950004577637),\n ('Ed_Miliband', 0.7125200033187866),\n ('chancellor_Alistair_Darling', 0.7029644846916199),\n ('Ed_Balls', 0.7017458081245422)]"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Try it with people\n",
    "wv.most_similar(positive=['Boris_Johnson','Gordon_Brown'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "c1bOgIIR81we",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[('snacks', 0.6008167266845703),\n ('crisps', 0.5351516604423523),\n ('snacking', 0.5131285786628723),\n ('snack_foods', 0.4977743625640869),\n ('Snacks', 0.48596659302711487),\n ('potato_chips', 0.4819340705871582),\n ('Snack', 0.47909489274024963),\n ('breakfast_cereal', 0.47190386056900024),\n ('lunch', 0.4588703215122223),\n ('yogurt', 0.45873087644577026)]"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try answering a question via adding vectors: What are some snacks that people like in England?\n",
    "wv.most_similar(positive=['England','snack'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "mwwWjY1L81wf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[('geese', 0.7546983957290649),\n ('Canada_geese', 0.6291584968566895),\n ('Geese', 0.6107496619224548),\n ('waterfowl', 0.6093072295188904),\n ('birds', 0.6005197763442993),\n ('mallards', 0.5957626104354858),\n ('pheasants', 0.5831050872802734),\n ('turkeys', 0.5781581401824951),\n ('moose', 0.5723955631256104),\n ('wild_turkeys', 0.572321891784668)]"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's experiment with subtracting vectors...\n",
    "# the \"positive\" words point in the direction(s) you want to go; the \"negative\" words' vectors are subtracted from these\n",
    "\n",
    "#Example from powerpoint: goose + (dogs - dog)\n",
    "wv.most_similar(positive=['goose','dogs'],negative=['dog']) #It's geese!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "z4H6D9_-81wf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[('slow', 0.5326714515686035),\n ('slower', 0.5169233679771423),\n ('painfully_slow', 0.42606601119041443),\n ('sluggish', 0.4135197401046753),\n ('fast', 0.4066060781478882),\n ('slowly', 0.38395875692367554),\n ('Slow', 0.38121846318244934),\n ('slower_pace', 0.35940873622894287),\n ('pace', 0.3565506935119629),\n ('subpar', 0.3554513156414032)]"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Another way of phrasing the above: Dog is to dogs as goose is to what?\n",
    "\n",
    "#Let's try some more\n",
    "# parts of speech: \"Longest is to long as slowest is to what?\"\n",
    "wv.most_similar(positive=['slowest','long'],negative=['longest'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "1XKxHYG981wf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[('queen', 0.7118192911148071),\n ('monarch', 0.6189674735069275),\n ('princess', 0.5902431011199951),\n ('crown_prince', 0.5499460697174072),\n ('prince', 0.5377321243286133),\n ('kings', 0.5236844420433044),\n ('queens', 0.5181134343147278),\n ('sultan', 0.5098593235015869),\n ('monarchy', 0.5087411403656006),\n ('royal_palace', 0.5087166428565979)]"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gender:\"man is to king as as woman is to what?\"\n",
    "wv.most_similar(positive=['woman','king'],negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "hOjNvCsq81wg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[('gynecologist', 0.7093892097473145),\n ('nurse', 0.647728681564331),\n ('doctors', 0.6471461057662964),\n ('physician', 0.6438996195793152),\n ('pediatrician', 0.6249487996101379),\n ('nurse_practitioner', 0.6218313574790955),\n ('obstetrician', 0.6072014570236206),\n ('ob_gyn', 0.5986712574958801),\n ('midwife', 0.5927063226699829),\n ('dermatologist', 0.5739566683769226)]"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#'man' minus 'woman' gives us a 'manly' vector. What happens when we subtract this vector from 'doctor'? \n",
    "wv.most_similar(positive=['woman','doctor'],negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "MrwLNlPL81wg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[('doctor', 0.8413018584251404),\n ('physician', 0.6823903918266296),\n ('doctors', 0.6239281892776489),\n ('surgeon', 0.5908077359199524),\n ('dentist', 0.570309042930603),\n ('cardiologist', 0.5666104555130005),\n ('neurologist', 0.5558010339736938),\n ('neurosurgeon', 0.5432174801826477),\n ('internist', 0.5405333042144775),\n ('urologist', 0.5398820042610168)]"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#'man' minus 'woman' gives us a 'manly' vector. What happens when we ADD this vector to 'doctor'? \n",
    "# Can do this explicitly in vector math and get nearly the same result as most_similar function\n",
    "X = wv['man'] - wv['woman']\n",
    "v = (wv['doctor'] + X)\n",
    "wv.similar_by_vector(v) #grabs the most similar words to a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "q76fVVdC81wg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[('Paris', 0.748170018196106),\n ('Marseille', 0.5542371273040771),\n ('Marseilles', 0.5350722670555115),\n ('Bordeaux', 0.5314581990242004),\n ('Reims', 0.5264857411384583),\n ('Saint_Denis', 0.5239812135696411),\n ('Madrid', 0.5148749351501465),\n ('Lyon', 0.5130605697631836),\n ('French', 0.5105733275413513),\n ('Aix_en_Provence', 0.5064885020256042)]"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Places: \"UK is to London as as France is to what?\"\n",
    "wv.most_similar(positive=['France','London'],negative=['UK'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "PhuF7cTo81wg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[('croissant', 0.47772619128227234),\n ('scone', 0.46981364488601685),\n ('crisps', 0.4671917259693146),\n ('muffin', 0.45740562677383423),\n ('muesli', 0.44982248544692993),\n ('marmalade', 0.4486430287361145),\n ('puddings', 0.44180625677108765),\n ('lasagne', 0.43633562326431274),\n ('veg', 0.43387067317962646),\n ('kebab', 0.43329814076423645)]"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#What is the bagel of London?\n",
    "wv.most_similar(positive=['London','bagel'],negative=['New_York'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "9_Z7ewvR81wg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[('Edith_Piaf', 0.4327777922153473),\n ('Teenage_Cancer', 0.410092830657959),\n ('Nina_Simone', 0.40169721841812134),\n ('Music', 0.3946076035499573),\n ('musicians', 0.39395079016685486),\n ('tunes', 0.3932993710041046),\n ('Piaf', 0.39279234409332275),\n ('songs', 0.38816142082214355),\n ('Melodies', 0.3843925893306732),\n ('classical_music', 0.3838009536266327)]"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Who is the Marie Curie of music?\n",
    "wv.most_similar(positive=['Marie_Curie', 'music'],negative=['science'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "YbMKVh7I81wg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'platypus'"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finally, can search for things that don't match:\n",
    "wv.doesnt_match(\"dog cat lion rhino platypus\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "U0-LQ9dp81wh",
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25472283\n",
      "0.441356\n"
     ]
    }
   ],
   "source": [
    "#More exploration of bias\n",
    "print(wv.similarity('man', 'nurse'))\n",
    "print(wv.similarity('woman','nurse'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "DqhkH6SL81wh",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12355876\n",
      "0.07830421\n"
     ]
    }
   ],
   "source": [
    "print(wv.similarity('man', 'intelligent'))\n",
    "print(wv.similarity('woman', 'intelligent'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "xGzZPqzr81wh",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18578778\n",
      "0.12420672\n"
     ]
    }
   ],
   "source": [
    "print(wv.similarity('man', 'competent'))\n",
    "print(wv.similarity('woman', 'competent'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "_sER97Dq81wh",
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0008112781\n",
      "0.07724926\n"
     ]
    }
   ],
   "source": [
    "print(wv.similarity('man', 'attractive'))\n",
    "print(wv.similarity('woman', 'attractive'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NsngUU2L81wh",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Keep playing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qKgWOEun81wh",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP Week 4.2-Word2Vec.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}